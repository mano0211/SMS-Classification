{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "222718d6",
   "metadata": {},
   "source": [
    "# ðŸ“§ SMS Spam Classifier â€“ Full Pipeline (Colab Ready)\n",
    "\n",
    "This notebook implements an endâ€‘toâ€‘end **SMS spam detection** project:\n",
    "\n",
    "- Load & clean the UCI SMS Spam dataset  \n",
    "- Text preprocessing + TFâ€‘IDF feature extraction  \n",
    "- Train multiple ML models (MultinomialNB, Logistic Regression, Linear SVM)  \n",
    "- Hyperparameter tuning with `RandomizedSearchCV`  \n",
    "- Model evaluation (Accuracy, Precision, Recall, F1, ROCâ€‘AUC)  \n",
    "- Graphs for model performance (ROC curves, F1 comparison, metric bars)  \n",
    "- `classify_sms()` helper  \n",
    "- Simple interactive textbox (ipywidgets)  \n",
    "- FastAPI app exercised via `TestClient` (RESTâ€‘style API inside Colab)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f45bda",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, string, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, classification_report,\n",
    "    confusion_matrix, roc_auc_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "UCI_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "LOCAL_DATA_PATH = None  # if you already have SMSSpamCollection TSV, put its path here\n",
    "\n",
    "SAVE_DIR = \"/content/artifacts_sms_spam\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Environment ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029027b",
   "metadata": {},
   "source": [
    "## 2. Load UCI SMS Spam Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c2926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, io, requests\n",
    "\n",
    "def load_uci_sms_spam(local_path: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Return DataFrame with columns ['label', 'message'].\n",
    "\n",
    "    If local_path is provided, read it as TSV. Otherwise, download from UCI.\n",
    "    \"\"\"\n",
    "    if local_path and os.path.exists(local_path):\n",
    "        df = pd.read_csv(local_path, sep=\"\\t\", header=None, names=[\"label\", \"message\"], encoding=\"utf-8\")\n",
    "        return df\n",
    "    \n",
    "    r = requests.get(UCI_URL, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    with z.open(\"SMSSpamCollection\") as f:\n",
    "        df = pd.read_csv(f, sep=\"\\t\", header=None, names=[\"label\", \"message\"], encoding=\"utf-8\")\n",
    "    return df\n",
    "\n",
    "df = load_uci_sms_spam(LOCAL_DATA_PATH)\n",
    "df = df.dropna(subset=[\"label\", \"message\"]).copy()\n",
    "df[\"label\"] = df[\"label\"].str.strip().str.lower()\n",
    "\n",
    "assert set(df[\"label\"].unique()) <= {\"ham\", \"spam\"}, \"Unexpected labels present!\"\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414659f0",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa84bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE = re.compile(r\"(https?://\\S+|www\\.\\S+)\", flags=re.IGNORECASE)\n",
    "HTML_RE = re.compile(r\"<.*?>\")\n",
    "NUM_RE  = re.compile(r\"\\d+\")\n",
    "PUNCT_TABLE = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = s.strip().lower()\n",
    "    s = URL_RE.sub(\" URL \", s)\n",
    "    s = HTML_RE.sub(\" \", s)\n",
    "    s = NUM_RE.sub(\" NUM \", s)\n",
    "    s = s.translate(PUNCT_TABLE)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "df[\"message_clean\"] = df[\"message\"].astype(str).apply(clean_text)\n",
    "df[[\"label\", \"message_clean\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c935e10",
   "metadata": {},
   "source": [
    "## 4. Train / Validation / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5807b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"message_clean\"].values\n",
    "y = (df[\"label\"].values == \"spam\").astype(int)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=RANDOM_STATE, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)}, Valid: {len(X_valid)}, Test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ed0e7c",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Feature Extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a065f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_word = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.98,\n",
    "    max_features=200_000\n",
    ")\n",
    "\n",
    "tfidf_char = TfidfVectorizer(\n",
    "    analyzer=\"char_wb\",\n",
    "    ngram_range=(3, 5),\n",
    "    min_df=2,\n",
    "    max_features=200_000\n",
    ")\n",
    "\n",
    "print(\"Vectorizers ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a40c5",
   "metadata": {},
   "source": [
    "## 6. Model Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1164a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {}\n",
    "\n",
    "pipelines[\"mnb_word\"] = Pipeline([\n",
    "    (\"tfidf\", tfidf_word),\n",
    "    (\"clf\", MultinomialNB(alpha=0.5))\n",
    "])\n",
    "\n",
    "pipelines[\"lr_char\"] = Pipeline([\n",
    "    (\"tfidf\", tfidf_char),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"liblinear\",\n",
    "        penalty=\"l2\",\n",
    "        C=2.0,\n",
    "        max_iter=200,\n",
    "        class_weight=\"balanced\",\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipelines[\"svm_char_calibrated\"] = Pipeline([\n",
    "    (\"tfidf\", tfidf_char),\n",
    "    (\"svm_cal\", CalibratedClassifierCV(\n",
    "        LinearSVC(C=1.0, random_state=RANDOM_STATE),\n",
    "        method=\"sigmoid\",\n",
    "        cv=3\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Base pipelines:\", list(pipelines.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6e57ac",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning â€“ Logistic Regression (RandomizedSearchCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c8e07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    \"clf__C\": np.logspace(-2, 1.3, 12),\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    \"clf__solver\": [\"liblinear\"],\n",
    "}\n",
    "\n",
    "lr_search = RandomizedSearchCV(\n",
    "    estimator=Pipeline([\n",
    "        (\"tfidf\", tfidf_char),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=300,\n",
    "            random_state=RANDOM_STATE\n",
    "        ))\n",
    "    ]),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=12,\n",
    "    scoring=\"f1\",\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "print(\"Running LR RandomizedSearchCV ...\")\n",
    "lr_search.fit(X_train, y_train)\n",
    "pipelines[\"lr_char_tuned\"] = lr_search.best_estimator_\n",
    "\n",
    "print(\"Best LR params:\", lr_search.best_params_)\n",
    "print(\"Pipelines now:\", list(pipelines.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee1754c",
   "metadata": {},
   "source": [
    "## 8. Evaluate Models on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f45eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(name, pipe, X_tr, y_tr, X_va, y_va):\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    preds = pipe.predict(X_va)\n",
    "    \n",
    "    try:\n",
    "        proba = pipe.predict_proba(X_va)[:, 1]\n",
    "    except Exception:\n",
    "        try:\n",
    "            scores = pipe.decision_function(X_va)\n",
    "            proba = MinMaxScaler().fit_transform(scores.reshape(-1,1)).ravel()\n",
    "        except Exception:\n",
    "            proba = None\n",
    "    \n",
    "    acc = accuracy_score(y_va, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y_va, preds, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    auc_score = roc_auc_score(y_va, proba) if proba is not None else np.nan\n",
    "    \n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy : {acc:.4f}\")\n",
    "    print(f\"Precision: {p:.4f} | Recall: {r:.4f} | F1: {f1:.4f}\")\n",
    "    if proba is not None:\n",
    "        print(f\"ROC-AUC : {auc_score:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_va, preds, target_names=[\"ham\",\"spam\"], zero_division=0))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_va, preds))\n",
    "    \n",
    "    return {\"name\": name, \"pipe\": pipe, \"acc\": acc, \"f1\": f1, \"auc\": auc_score}\n",
    "\n",
    "results = []\n",
    "for name, pipe in pipelines.items():\n",
    "    results.append(eval_model(name, pipe, X_train, y_train, X_valid, y_valid))\n",
    "\n",
    "print(\"\\nDone evaluating all models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00064a74",
   "metadata": {},
   "source": [
    "## 9. Pick Best Model & Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafc92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results)\n",
    "res_df[\"auc_rank\"] = res_df[\"auc\"].rank(ascending=False, method=\"min\")\n",
    "res_df[\"f1_rank\"] = res_df[\"f1\"].rank(ascending=False, method=\"min\")\n",
    "res_df[\"score\"] = 2*res_df[\"f1_rank\"] + res_df[\"auc_rank\"]\n",
    "\n",
    "leaderboard = res_df.sort_values([\"score\", \"f1\", \"auc\"], ascending=[True, False, False])\n",
    "best_row = leaderboard.iloc[0]\n",
    "best_model = best_row[\"pipe\"]\n",
    "\n",
    "print(\"Leaderboard (Validation):\")\n",
    "display(leaderboard[[\"name\", \"acc\", \"f1\", \"auc\", \"score\"]])\n",
    "\n",
    "print(\"\\nSelected best model:\", best_row[\"name\"])\n",
    "\n",
    "X_train_full = np.concatenate([X_train, X_valid])\n",
    "y_train_full = np.concatenate([y_train, y_valid])\n",
    "best_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "test_preds = best_model.predict(X_test)\n",
    "\n",
    "try:\n",
    "    test_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "except Exception:\n",
    "    try:\n",
    "        scores = best_model.decision_function(X_test)\n",
    "        test_proba = MinMaxScaler().fit_transform(scores.reshape(-1,1)).ravel()\n",
    "    except Exception:\n",
    "        test_proba = None\n",
    "\n",
    "test_acc = accuracy_score(y_test, test_preds)\n",
    "test_p, test_r, test_f1, _ = precision_recall_fscore_support(\n",
    "    y_test, test_preds, average=\"binary\", zero_division=0\n",
    ")\n",
    "test_auc = roc_auc_score(y_test, test_proba) if test_proba is not None else np.nan\n",
    "\n",
    "print(f\"\\n=== Final Test Evaluation (Best Model: {best_row['name']}) ===\")\n",
    "print(f\"Accuracy : {test_acc:.4f}\")\n",
    "print(f\"Precision: {test_p:.4f} | Recall: {test_r:.4f} | F1: {test_f1:.4f}\")\n",
    "if test_proba is not None:\n",
    "    print(f\"ROC-AUC : {test_auc:.4f}\")\n",
    "print(\"\\nClassification Report (Test):\\n\", classification_report(y_test, test_preds, target_names=[\"ham\",\"spam\"], zero_division=0))\n",
    "print(\"Confusion Matrix (Test):\\n\", confusion_matrix(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a4407",
   "metadata": {},
   "source": [
    "## 10. Visualizations â€“ Model Metrics & ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad4db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.1 F1 score comparison across models\n",
    "model_names = res_df[\"name\"].tolist()\n",
    "f1_scores = res_df[\"f1\"].tolist()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(model_names, f1_scores)\n",
    "plt.title(\"F1 Score Comparison Across Models (Validation)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.ylim(0, 1)\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.01, f\"{yval:.3f}\", ha=\"center\")\n",
    "plt.show()\n",
    "\n",
    "# 10.2 Best model metrics on test set\n",
    "metrics = {\n",
    "    \"Accuracy\": test_acc,\n",
    "    \"Precision\": test_p,\n",
    "    \"Recall\": test_r,\n",
    "    \"F1 Score\": test_f1\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "bars = plt.bar(list(metrics.keys()), list(metrics.values()))\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Best Model â€“ Test Metrics\")\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f\"{yval:.3f}\", ha=\"center\")\n",
    "plt.show()\n",
    "\n",
    "# 10.3 ROC curve for best model\n",
    "if test_proba is not None:\n",
    "    fpr, tpr, _ = roc_curve(y_test, test_proba)\n",
    "    roc_auc_val = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=f\"Best model (AUC = {roc_auc_val:.3f})\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve â€“ Best Model (Test)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 10.4 ROC comparison for all models\n",
    "plt.figure(figsize=(8, 6))\n",
    "for r in results:\n",
    "    name = r[\"name\"]\n",
    "    pipe = r[\"pipe\"]\n",
    "    pipe.fit(X_train_full, y_train_full)\n",
    "    try:\n",
    "        proba = pipe.predict_proba(X_test)[:, 1]\n",
    "    except Exception:\n",
    "        try:\n",
    "            scores = pipe.decision_function(X_test)\n",
    "            proba = MinMaxScaler().fit_transform(scores.reshape(-1,1)).ravel()\n",
    "        except Exception:\n",
    "            continue\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    roc_auc_all = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, linewidth=1.5, label=f\"{name} (AUC={roc_auc_all:.3f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison â€“ All Models (Test)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8aa9566",
   "metadata": {},
   "source": [
    "## 11. Save Best Model & Helper Function `classify_sms`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bce436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "MODEL_PATH = os.path.join(SAVE_DIR, f\"best_model_{best_row['name']}.joblib\")\n",
    "joblib.dump(best_model, MODEL_PATH)\n",
    "print(\"Saved best model to:\", MODEL_PATH)\n",
    "\n",
    "def classify_sms(messages):\n",
    "    \"\"\"Classify a single SMS string or list of strings.\"\"\"\n",
    "    single = False\n",
    "    if isinstance(messages, str):\n",
    "        messages = [messages]\n",
    "        single = True\n",
    "    \n",
    "    cleaned = [clean_text(m) for m in messages]\n",
    "    preds = best_model.predict(cleaned)\n",
    "    \n",
    "    try:\n",
    "        proba = best_model.predict_proba(cleaned)[:, 1]\n",
    "    except Exception:\n",
    "        try:\n",
    "            scores = best_model.decision_function(cleaned)\n",
    "            proba = MinMaxScaler().fit_transform(scores.reshape(-1,1)).ravel()\n",
    "        except Exception:\n",
    "            proba = np.zeros(len(cleaned))\n",
    "    \n",
    "    out = []\n",
    "    for m, p, s in zip(messages, preds, proba):\n",
    "        out.append({\n",
    "            \"input\": m,\n",
    "            \"pred_label\": \"spam\" if p == 1 else \"ham\",\n",
    "            \"spam_probability\": float(s)\n",
    "        })\n",
    "    return out[0] if single else out\n",
    "\n",
    "print(\"\\nExample classifications:\")\n",
    "print(classify_sms(\"WINNER!! You have won a free ticket, reply NOW!\"))\n",
    "print(classify_sms(\"Hey, are we still on for lunch at 12?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a5742",
   "metadata": {},
   "source": [
    "## 12. Interactive SMS Classifier (ipywidgets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d565efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import Textarea, Button, VBox, Output, HBox\n",
    "from IPython.display import display\n",
    "\n",
    "sms_input = Textarea(\n",
    "    value=\"WINNER!! You have won a free ticket to Bahamas, reply NOW to claim\",\n",
    "    placeholder=\"Type an SMS message here...\",\n",
    "    description=\"SMS:\",\n",
    "    layout={\"width\": \"100%\", \"height\": \"80px\"},\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "classify_button = Button(\n",
    "    description=\"Classify\",\n",
    "    disabled=False,\n",
    "    tooltip=\"Click to classify this SMS as ham/spam\",\n",
    ")\n",
    "\n",
    "out = Output()\n",
    "\n",
    "def on_classify_clicked(b):\n",
    "    out.clear_output()\n",
    "    text = sms_input.value.strip()\n",
    "    if not text:\n",
    "        with out:\n",
    "            print(\"Please type a message first.\")\n",
    "        return\n",
    "    result = classify_sms(text)\n",
    "    with out:\n",
    "        print(\"Input:\", result[\"input\"])\n",
    "        print(\"Predicted label:\", result[\"pred_label\"])\n",
    "        print(\"Spam probability:\", f\"{result['spam_probability']:.4f}\")\n",
    "\n",
    "classify_button.on_click(on_classify_clicked)\n",
    "\n",
    "ui = VBox([sms_input, HBox([classify_button]), out])\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbbc938",
   "metadata": {},
   "source": [
    "## 13. FastAPI TestClient â€“ REST-style API Inside Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ce1ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install fastapi\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from fastapi.testclient import TestClient\n",
    "\n",
    "app = FastAPI(title=\"SMS Spam Classifier API (Colab Demo)\")\n",
    "\n",
    "class SMSRequest(BaseModel):\n",
    "    message: str\n",
    "\n",
    "class SMSResponse(BaseModel):\n",
    "    input: str\n",
    "    pred_label: str\n",
    "    spam_probability: float\n",
    "\n",
    "@app.post(\"/predict\", response_model=SMSResponse)\n",
    "def predict_sms(req: SMSRequest):\n",
    "    result = classify_sms(req.message)\n",
    "    return SMSResponse(\n",
    "        input=result[\"input\"],\n",
    "        pred_label=result[\"pred_label\"],\n",
    "        spam_probability=result[\"spam_probability\"],\n",
    "    )\n",
    "\n",
    "client = TestClient(app)\n",
    "print(\"FastAPI app and TestClient ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8fba8d",
   "metadata": {},
   "source": [
    "### 13.1 Test the `/predict` Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccbb46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"message\": \"WINNER!! You have won a free ticket to Bahamas, reply NOW to claim\"}\n",
    "r = client.post(\"/predict\", json=payload)\n",
    "print(\"Status code:\", r.status_code)\n",
    "print(\"JSON response:\", r.json())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
